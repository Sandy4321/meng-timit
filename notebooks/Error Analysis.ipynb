{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../cnn\")\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import seaborn as sns; sns.set()\n",
    "\n",
    "from cnn_md import CNNMultidecoder, CNNVariationalMultidecoder\n",
    "from cnn_md import CNNAdversarialMultidecoder\n",
    "from cnn_md import CNNGANMultidecoder\n",
    "from hao_data import HaoEvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise ratio 0.25\n",
      "Using CNN name ENC_C_256_256_K_3_3_P_3_3_F_/LATENT_256/DEC_F__C_256_256_K_3_3_P_3_3/ACT_ReLU_BN_false_WEIGHT_INIT_xavier_uniform/OPT_Adam_LR_0.0001_EPOCHS_25_BATCH_256_DEBUG_false\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables for the model we want to examine\n",
    "# NOT necessarily the current environment variables!!\n",
    "\n",
    "feat_dim=80\n",
    "left_context=5\n",
    "right_context=5\n",
    "\n",
    "optimizer=\"Adam\"\n",
    "learning_rate=\"0.0001\" # Use string instead of float to prevent Python's auto-formatting...\n",
    "epochs=25\n",
    "batch_size=256\n",
    "\n",
    "enc_channels=[256, 256]\n",
    "enc_kernels=[3, 3]\n",
    "enc_pools=[3, 3]\n",
    "enc_fc=[]\n",
    "\n",
    "latent_dim=256\n",
    "\n",
    "dec_fc=[]\n",
    "dec_channels=[256, 256]\n",
    "dec_kernels=[3, 3]\n",
    "dec_pools=[3, 3]\n",
    "\n",
    "decoder_classes=[\"ihm\", \"sdm1\"]\n",
    "\n",
    "use_batch_norm=False\n",
    "use_batch_norm_str = \"true\" if use_batch_norm else \"false\"\n",
    "activation=\"ReLU\"\n",
    "weight_init=\"xavier_uniform\"\n",
    "\n",
    "enc_channels_delim=\"_\" + \"_\".join(map(str, enc_channels))\n",
    "if len(enc_channels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_channels_delim=\"_\"\n",
    "enc_kernels_delim=\"_\" + \"_\".join(map(str, enc_kernels))\n",
    "if len(enc_kernels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_kernels_delim=\"_\"\n",
    "enc_pools_delim=\"_\" + \"_\".join(map(str, enc_pools))\n",
    "if len(enc_pools) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_pools_delim=\"_\"\n",
    "enc_fc_delim=\"_\" + \"_\".join(map(str, enc_fc))\n",
    "if len(enc_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_fc_delim=\"_\"\n",
    "    \n",
    "dec_fc_delim=\"_\" + \"_\".join(map(str, dec_fc))\n",
    "if len(dec_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_fc_delim=\"_\"\n",
    "dec_channels_delim=\"_\" + \"_\".join(map(str, dec_channels))\n",
    "if len(dec_channels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_channels_delim=\"_\"\n",
    "dec_kernels_delim=\"_\" + \"_\".join(map(str, dec_kernels))\n",
    "if len(dec_kernels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_kernels_delim=\"_\"\n",
    "dec_pools_delim=\"_\" + \"_\".join(map(str, dec_pools))\n",
    "if len(dec_pools) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_pools_delim=\"_\"\n",
    "\n",
    "debug_model = False\n",
    "debug_str = \"true\" if debug_model else \"false\"\n",
    "cnn_name = \"ENC_C%s_K%s_P%s_F%s/LATENT_%d/DEC_F%s_C%s_K%s_P%s/ACT_%s_BN_%s_WEIGHT_INIT_%s/OPT_%s_LR_%s_EPOCHS_%d_BATCH_%d_DEBUG_%s\" % (enc_channels_delim,\n",
    "                                                                                                                                        enc_kernels_delim, \n",
    "                                                                                                                                        enc_pools_delim, \n",
    "                                                                                                                                        enc_fc_delim,\n",
    "                                                                                                                                        latent_dim,\n",
    "                                                                                                                                        dec_fc_delim,\n",
    "                                                                                                                                        dec_channels_delim,\n",
    "                                                                                                                                        dec_kernels_delim,                                                                                                                   dec_pools_delim,\n",
    "                                                                                                                                        activation,\n",
    "                                                                                                                                        use_batch_norm_str,\n",
    "                                                                                                                                        weight_init,\n",
    "                                                                                                                                        optimizer,\n",
    "                                                                                                                                        learning_rate,\n",
    "                                                                                                                                        epochs,\n",
    "                                                                                                                                        batch_size,\n",
    "                                                                                                                                        debug_str)\n",
    "\n",
    "time_dim = (left_context + right_context + 1)\n",
    "freq_dim = feat_dim\n",
    "\n",
    "noise_ratio=0.25\n",
    "print(\"Noise ratio %s\" % str(noise_ratio))\n",
    "\n",
    "print(\"Using CNN name %s\" % cnn_name)\n",
    "\n",
    "dataset = \"ami-0.1\"\n",
    "\n",
    "adv_fc = [256]\n",
    "adv_fc_delim=\"_\" + \"_\".join(map(str, adv_fc))\n",
    "if len(adv_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    adv_fc_delim=\"_\"\n",
    "adv_activation = \"Sigmoid\"\n",
    "\n",
    "gan_fc = [256]\n",
    "gan_fc_delim=\"_\" + \"_\".join(map(str, gan_fc))\n",
    "if len(adv_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    gan_fc_delim=\"_\"\n",
    "gan_activation = \"Sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up baseline dev datasets\n",
      "Set up augmented dev datasets\n",
      "Done data setup\n"
     ]
    }
   ],
   "source": [
    "# Set up datasets + error arrays for IHM, SDM1 baselines (dev set only)\n",
    "dataset_name = \"ami-0.1\"\n",
    "\n",
    "feat_dir = \"/data/sls/r/u/atitus5/meng/%s\" % dataset_name\n",
    "am_logs = \"/data/sls/r/u/atitus5/meng/am/logs/%s\" % dataset_name\n",
    "\n",
    "baseline_feats = dict()\n",
    "baseline_errs = {decoder_class: dict() for decoder_class in decoder_classes}\n",
    "for decoder_class in decoder_classes:\n",
    "    baseline_feats[decoder_class] = HaoEvalDataset(os.path.join(feat_dir,\n",
    "                                                                \"%s-dev-norm.blogmel.scp\" % decoder_class))\n",
    "    \n",
    "    expt_name = \"train_%s/baseline/frame-tdnn-450x7-step0.05\" % decoder_class\n",
    "    err_dir = os.path.join(am_logs, expt_name)\n",
    "    for predict_domain in decoder_classes:\n",
    "        baseline_errs[decoder_class][predict_domain] = HaoEvalDataset(os.path.join(err_dir, \"predict_%s/errors.scp\" % predict_domain))\n",
    "\n",
    "print(\"Set up baseline dev datasets\")\n",
    "\n",
    "# Load in error arrays for IHM->IHM, IHM->SDM1, SDM1->SDM1, SDM1->IHM (dev set only)\n",
    "model_type = \"ae\"\n",
    "adversarial = False\n",
    "gan = False\n",
    "\n",
    "augmented_errs = {decoder_class: {decoder_class_2: dict() for decoder_class_2 in decoder_classes} for decoder_class in decoder_classes}\n",
    "for source_domain in decoder_classes:\n",
    "    for target_domain in decoder_classes:\n",
    "        expt_name = \"train_%s/augmented_src_%s/frame-tdnn-450x7-step0.05/%s\" % (target_domain,\n",
    "                                                                                source_domain,\n",
    "                                                                                cnn_name)\n",
    "        err_base_dir = os.path.join(am_logs, expt_name)\n",
    "        \n",
    "        if adversarial:\n",
    "            err_dir = os.path.join(err_base_dir, \"adversarial_fc_%s_act_%s_%s_ratio%s\" % (adv_fc_delim,\n",
    "                                                                                          adv_activation,\n",
    "                                                                                          model_type,\n",
    "                                                                                          str(noise_ratio)))\n",
    "        elif gan:\n",
    "            err_dir = os.path.join(err_base_dir, \"gan_fc_%s_act_%s_%s_ratio%s\" % (gan_fc_delim,\n",
    "                                                                                  gan_activation,\n",
    "                                                                                  model_type,\n",
    "                                                                                  str(noise_ratio)))\n",
    "        else:\n",
    "            '''\n",
    "            err_dir = os.path.join(err_base_dir, \"%s_ratio%s\" % (model_type,\n",
    "                                                                 str(noise_ratio)))\n",
    "            '''\n",
    "            err_dir = os.path.join(err_base_dir, \"%s\" % model_type)\n",
    "            \n",
    "        for predict_domain in decoder_classes:\n",
    "            augmented_errs[source_domain][target_domain][predict_domain] = HaoEvalDataset(os.path.join(err_dir,\n",
    "                                                                                                       \"predict_%s/errors.scp\" % predict_domain))\n",
    "\n",
    "print(\"Set up augmented dev datasets\")\n",
    "\n",
    "print(\"Done data setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68192c5c5d3546f6a89d315a10170611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotParallelUttErrors>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# color_map = \"coolwarm\"\n",
    "color_map = \"viridis\"\n",
    "idx_to_domain = [\"ihm\", \"sdm1\"]\n",
    "def plotParallelUttErrors(utt_id_idx):\n",
    "    plt.clf()\n",
    "    fig, axarr = plt.subplots(7, len(idx_to_domain), sharex=True)\n",
    "    fig.set_size_inches(8.5, 11)\n",
    "    \n",
    "    for i in range(len(idx_to_domain)):\n",
    "        predict_domain = idx_to_domain[i]\n",
    "    \n",
    "        # First show log mels for baseline\n",
    "        utt_id = baseline_feats[predict_domain].utt_ids[utt_id_idx]\n",
    "        feats = baseline_feats[predict_domain].feats_for_uttid(utt_id)\n",
    "        axarr[0, i].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "        axarr[0, i].imshow(np.transpose(feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "        axarr[0, i].set_title(\"%s baseline feats\" % predict_domain)\n",
    "        \n",
    "        # Show errors for baseline in- and out-domain\n",
    "        row_idx = 1\n",
    "        for source_domain in idx_to_domain:\n",
    "            errs = baseline_errs[source_domain][predict_domain].feats_for_uttid(utt_id)\n",
    "            axarr[row_idx, i].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "            axarr[row_idx, i].imshow(errs, origin='lower', cmap=\"RdYlGn\", aspect='auto', interpolation='none')\n",
    "            axarr[row_idx, i].set_title(\"%s baseline AM errors for %s\" % (source_domain, predict_domain))\n",
    "            row_idx += 1\n",
    "            \n",
    "        # Show errors for augmented data AMs\n",
    "        for source_domain in idx_to_domain:\n",
    "            for target_domain in idx_to_domain:\n",
    "                '''\n",
    "                adj_utt_id = \"src_%s_tar_%s_%s\" % (source_domain, target_domain, utt_id)\n",
    "                errs = augmented_errs[source_domain][target_domain][predict_domain].feats_for_uttid(adj_utt_id)\n",
    "                '''\n",
    "                errs = augmented_errs[source_domain][target_domain][predict_domain].feats_for_uttid(utt_id)\n",
    "                \n",
    "                axarr[row_idx, i].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "                axarr[row_idx, i].imshow(errs, origin='lower', cmap=\"RdYlGn\", aspect='auto', interpolation='none')\n",
    "                axarr[row_idx, i].set_title(\"%s->%s baseline AM errors for %s\" % (source_domain, target_domain, predict_domain))\n",
    "                row_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(\"%s_idx%d.eps\" % (utt_id, utt_id_idx))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 43, 251 are good utterances (long + variations between IHM and SDM1)\n",
    "interact(plotParallelUttErrors, utt_id_idx=range(len(baseline_feats[\"ihm\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
