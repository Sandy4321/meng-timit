{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../cnn\")\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import seaborn as sns; sns.set()\n",
    "\n",
    "from cnn_md import CNNMultidecoder, CNNVariationalMultidecoder\n",
    "from cnn_md import CNNDomainAdversarialMultidecoder\n",
    "from cnn_md import CNNGANMultidecoder\n",
    "from kaldi_data import KaldiEvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise ratio 0.0\n",
      "Using experiment BACKTRANS_false_ENC_C_64_64_K_3_3_P_2_2_F_/LATENT_256/DEC_F__C_64_64_K_3_3_P_2_2/ACT_ReLU_BN_false_WEIGHT_INIT_xavier_uniform/OPT_Adam_LR_0.001_L2_REG_0.0_EPOCHS_100_BATCH_32\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables for the model we want to examine\n",
    "# NOT necessarily the current environment variables!!\n",
    "\n",
    "feat_dim=40\n",
    "left_context=5\n",
    "right_context=5\n",
    "\n",
    "optimizer=\"Adam\"\n",
    "learning_rate=\"0.001\" # Use string instead of float to prevent Python's auto-formatting...\n",
    "l2_reg=\"0.0\" # Use string instead of float to prevent Python's auto-formatting...\n",
    "epochs=100\n",
    "batch_size=32\n",
    "\n",
    "enc_channels=[64, 64]\n",
    "enc_kernels=[3, 3]\n",
    "enc_downsamples=[2, 2]\n",
    "enc_fc=[]\n",
    "\n",
    "latent_dim=256\n",
    "\n",
    "dec_fc=[]\n",
    "dec_channels=[64, 64]\n",
    "dec_kernels=[3, 3]\n",
    "dec_upsamples=[2, 2]\n",
    "\n",
    "decoder_classes=[\"clean\", \"dirty\"]\n",
    "\n",
    "use_batch_norm=False\n",
    "use_batch_norm_str = \"true\" if use_batch_norm else \"false\"\n",
    "activation=\"ReLU\"\n",
    "weight_init=\"xavier_uniform\"\n",
    "\n",
    "enc_channels_delim=\"_\" + \"_\".join(map(str, enc_channels))\n",
    "if len(enc_channels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_channels_delim=\"_\"\n",
    "enc_kernels_delim=\"_\" + \"_\".join(map(str, enc_kernels))\n",
    "if len(enc_kernels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_kernels_delim=\"_\"\n",
    "enc_downsamples_delim=\"_\" + \"_\".join(map(str, enc_downsamples))\n",
    "if len(enc_downsamples) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_downsamples_delim=\"_\"\n",
    "enc_fc_delim=\"_\" + \"_\".join(map(str, enc_fc))\n",
    "if len(enc_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    enc_fc_delim=\"_\"\n",
    "    \n",
    "dec_fc_delim=\"_\" + \"_\".join(map(str, dec_fc))\n",
    "if len(dec_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_fc_delim=\"_\"\n",
    "dec_channels_delim=\"_\" + \"_\".join(map(str, dec_channels))\n",
    "if len(dec_channels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_channels_delim=\"_\"\n",
    "dec_kernels_delim=\"_\" + \"_\".join(map(str, dec_kernels))\n",
    "if len(dec_kernels) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_kernels_delim=\"_\"\n",
    "dec_upsamples_delim=\"_\" + \"_\".join(map(str, dec_upsamples))\n",
    "if len(dec_upsamples) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    dec_upsamples_delim=\"_\"\n",
    "\n",
    "use_backtranslation = False\n",
    "use_backtranslation_str = \"true\" if use_backtranslation else \"false\"\n",
    "\n",
    "expt_name = \"BACKTRANS_%s_ENC_C%s_K%s_P%s_F%s/LATENT_%d/DEC_F%s_C%s_K%s_P%s/ACT_%s_BN_%s_WEIGHT_INIT_%s/OPT_%s_LR_%s_L2_REG_%s_EPOCHS_%d_BATCH_%d\" % (use_backtranslation_str,\n",
    "                                                                                                                                        enc_channels_delim,\n",
    "                                                                                                                                        enc_kernels_delim, \n",
    "                                                                                                                                        enc_downsamples_delim, \n",
    "                                                                                                                                        enc_fc_delim,\n",
    "                                                                                                                                        latent_dim,\n",
    "                                                                                                                                        dec_fc_delim,\n",
    "                                                                                                                                        dec_channels_delim,\n",
    "                                                                                                                                        dec_kernels_delim,                                                                                                                   \n",
    "                                                                                                                                        dec_upsamples_delim,\n",
    "                                                                                                                                        activation,\n",
    "                                                                                                                                        use_batch_norm_str,\n",
    "                                                                                                                                        weight_init,\n",
    "                                                                                                                                        optimizer,\n",
    "                                                                                                                                        learning_rate,\n",
    "                                                                                                                                        l2_reg,\n",
    "                                                                                                                                        epochs,\n",
    "                                                                                                                                        batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_dim = (left_context + right_context + 1)\n",
    "freq_dim = feat_dim\n",
    "\n",
    "noise_ratio=0.0\n",
    "print(\"Noise ratio %s\" % str(noise_ratio))\n",
    "\n",
    "print(\"Using experiment %s\" % expt_name)\n",
    "\n",
    "clean_dataset = \"timit_clean\"\n",
    "dirty_dataset = \"timit_dirty_single_rir\"\n",
    "\n",
    "domain_adv_fc = [512, 512]\n",
    "domain_adv_fc_delim=\"_\" + \"_\".join(map(str, domain_adv_fc))\n",
    "if len(domain_adv_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    domain_adv_fc_delim=\"_\"\n",
    "domain_adv_activation = \"Sigmoid\"\n",
    "\n",
    "gan_fc = [512, 512]\n",
    "gan_fc_delim=\"_\" + \"_\".join(map(str, gan_fc))\n",
    "if len(gan_fc) == 0:\n",
    "     # need to recreate bash join behavior with empty array\n",
    "    gan_fc_delim=\"_\"\n",
    "gan_activation = \"Sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up baseline dev datasets\n"
     ]
    }
   ],
   "source": [
    "# Set up datasets for clean, dirty baselines (dev set only)\n",
    "clean_feat_dir = \"/data/sls/scratch/atitus5/meng/feats/%s\" % clean_dataset\n",
    "clean_baseline = KaldiEvalDataset(os.path.join(clean_feat_dir, \"dev.scp\"))\n",
    "\n",
    "dirty_feat_dir = \"/data/sls/scratch/atitus5/meng/feats/%s\" % dirty_dataset\n",
    "dirty_baseline = KaldiEvalDataset(os.path.join(dirty_feat_dir, \"dev.scp\"))\n",
    "\n",
    "print(\"Set up baseline dev datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF DATA AUGMENTATION NOT RUN YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNGANMultidecoder(\n",
       "  (encoder_conv): Sequential(\n",
       "    (conv2d_0): Conv2d (1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (ReLU_0): ReLU()\n",
       "    (maxpool2d_0): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), dilation=(1, 1))\n",
       "    (conv2d_1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (ReLU_1): ReLU()\n",
       "    (maxpool2d_1): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (encoder_fc): Sequential(\n",
       "    (lin_final): Linear(in_features=3584, out_features=256)\n",
       "  )\n",
       "  (decoder_fc_clean): Sequential(\n",
       "    (ReLU_final): ReLU()\n",
       "    (lin_final): Linear(in_features=256, out_features=3584)\n",
       "  )\n",
       "  (decoder_deconv_clean): Sequential(\n",
       "    (ReLU_0): ReLU()\n",
       "    (maxunpool2d_0): MaxUnpool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0))\n",
       "    (conv2d_0): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (ReLU_1): ReLU()\n",
       "    (maxunpool2d_1): MaxUnpool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0))\n",
       "    (conv2d_1): Conv2d (64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (decoder_fc_dirty): Sequential(\n",
       "    (ReLU_final): ReLU()\n",
       "    (lin_final): Linear(in_features=256, out_features=3584)\n",
       "  )\n",
       "  (decoder_deconv_dirty): Sequential(\n",
       "    (ReLU_0): ReLU()\n",
       "    (maxunpool2d_0): MaxUnpool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0))\n",
       "    (conv2d_0): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (ReLU_1): ReLU()\n",
       "    (maxunpool2d_1): MaxUnpool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0))\n",
       "    (conv2d_1): Conv2d (64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (gan_clean): Sequential(\n",
       "    (lin_0): Linear(in_features=440, out_features=512)\n",
       "    (Sigmoid_0): Sigmoid()\n",
       "    (lin_1): Linear(in_features=512, out_features=512)\n",
       "    (Sigmoid_1): Sigmoid()\n",
       "    (lin_final): Linear(in_features=512, out_features=1)\n",
       "    (Sigmoid_final): Sigmoid()\n",
       "  )\n",
       "  (gan_dirty): Sequential(\n",
       "    (lin_0): Linear(in_features=440, out_features=512)\n",
       "    (Sigmoid_0): Sigmoid()\n",
       "    (lin_1): Linear(in_features=512, out_features=512)\n",
       "    (Sigmoid_1): Sigmoid()\n",
       "    (lin_final): Linear(in_features=512, out_features=1)\n",
       "    (Sigmoid_final): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = \"ae\"\n",
    "domain_adversarial = False\n",
    "gan = True\n",
    "checkpoint_dir = \"/data/sls/scratch/atitus5/meng/models/cnn/%s/%s\" % (dirty_dataset,\n",
    "                                                                      expt_name)\n",
    "\n",
    "if domain_adversarial:\n",
    "    checkpoint_file = \"%s/best_cnn_domain_adversarial_fc_%s_act_%s_%s_ratio%s_md.pth.tar\" % (checkpoint_dir,\n",
    "                                                                      domain_adv_fc_delim,\n",
    "                                                                      domain_adv_activation,\n",
    "                                                                      model_type,\n",
    "                                                                      str(noise_ratio))\n",
    "    \n",
    "    if model_type == \"ae\":\n",
    "        model = CNNDomainAdversarialMultidecoder(freq_dim=freq_dim,\n",
    "                                splicing=[left_context, right_context], \n",
    "                                enc_channel_sizes=enc_channels,\n",
    "                                enc_kernel_sizes=enc_kernels,\n",
    "                                enc_downsample_sizes=enc_downsamples,\n",
    "                                enc_fc_sizes=enc_fc,\n",
    "                                latent_dim=latent_dim,\n",
    "                                dec_fc_sizes=dec_fc,\n",
    "                                dec_channel_sizes=dec_channels,\n",
    "                                dec_kernel_sizes=dec_kernels,\n",
    "                                dec_upsample_sizes=dec_upsamples,\n",
    "                                activation=activation,\n",
    "                                decoder_classes=decoder_classes,\n",
    "                                use_batch_norm=use_batch_norm,\n",
    "                                weight_init=weight_init,\n",
    "                                domain_adv_fc_sizes=domain_adv_fc,\n",
    "                                domain_adv_activation=domain_adv_activation)\n",
    "    elif model_type == \"vae\":\n",
    "        print(\"Adversarial VAE not supported yet\")\n",
    "elif gan:\n",
    "    checkpoint_file = \"%s/best_cnn_gan_fc_%s_act_%s_%s_ratio%s_md.pth.tar\" % (checkpoint_dir,\n",
    "                                                                      gan_fc_delim,\n",
    "                                                                      gan_activation,\n",
    "                                                                      model_type,\n",
    "                                                                      str(noise_ratio))\n",
    "    \n",
    "    if model_type == \"ae\":\n",
    "        model = CNNGANMultidecoder(freq_dim=freq_dim,\n",
    "                                splicing=[left_context, right_context], \n",
    "                                enc_channel_sizes=enc_channels,\n",
    "                                enc_kernel_sizes=enc_kernels,\n",
    "                                enc_downsample_sizes=enc_downsamples,\n",
    "                                enc_fc_sizes=enc_fc,\n",
    "                                latent_dim=latent_dim,\n",
    "                                dec_fc_sizes=dec_fc,\n",
    "                                dec_channel_sizes=dec_channels,\n",
    "                                dec_kernel_sizes=dec_kernels,\n",
    "                                dec_upsample_sizes=dec_upsamples,\n",
    "                                activation=activation,\n",
    "                                decoder_classes=decoder_classes,\n",
    "                                use_batch_norm=use_batch_norm,\n",
    "                                weight_init=weight_init,\n",
    "                                gan_fc_sizes=gan_fc,\n",
    "                                gan_activation=gan_activation)\n",
    "    elif model_type == \"vae\":\n",
    "        print(\"Generative adversarial VAE not supported yet\")\n",
    "else:\n",
    "    checkpoint_file = \"%s/best_cnn_%s_ratio%s_md.pth.tar\" % (checkpoint_dir,\n",
    "                                                             model_type,\n",
    "                                                             str(noise_ratio))\n",
    "    \n",
    "    if model_type == \"ae\":\n",
    "        model = CNNMultidecoder(freq_dim=freq_dim,\n",
    "                                splicing=[left_context, right_context], \n",
    "                                enc_channel_sizes=enc_channels,\n",
    "                                enc_kernel_sizes=enc_kernels,\n",
    "                                enc_downsample_sizes=enc_downsamples,\n",
    "                                enc_fc_sizes=enc_fc,\n",
    "                                latent_dim=latent_dim,\n",
    "                                dec_fc_sizes=dec_fc,\n",
    "                                dec_channel_sizes=dec_channels,\n",
    "                                dec_kernel_sizes=dec_kernels,\n",
    "                                dec_upsample_sizes=dec_upsamples,\n",
    "                                activation=activation,\n",
    "                                decoder_classes=decoder_classes,\n",
    "                                use_batch_norm=use_batch_norm,\n",
    "                                weight_init=weight_init)\n",
    "    elif model_type == \"vae\":\n",
    "        model = CNNVariationalMultidecoder(freq_dim=freq_dim,\n",
    "                                splicing=[left_context, right_context], \n",
    "                                enc_channel_sizes=enc_channels,\n",
    "                                enc_kernel_sizes=enc_kernels,\n",
    "                                enc_downsample_sizes=enc_downsamples,\n",
    "                                enc_fc_sizes=enc_fc,\n",
    "                                latent_dim=latent_dim,\n",
    "                                dec_fc_sizes=dec_fc,\n",
    "                                dec_channel_sizes=dec_channels,\n",
    "                                dec_kernel_sizes=dec_kernels,\n",
    "                                dec_upsample_sizes=dec_upsamples,\n",
    "                                activation=activation,\n",
    "                                decoder_classes=decoder_classes,\n",
    "                                use_batch_norm=use_batch_norm,\n",
    "                                weight_init=weight_init)\n",
    "        \n",
    "        \n",
    "checkpoint = torch.load(checkpoint_file, map_location=lambda storage,loc: storage)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augmentFeats(model, feats, decoder_class):\n",
    "    feats_numpy = feats.reshape((-1, freq_dim))\n",
    "    num_frames = feats_numpy.shape[0]\n",
    "    decoded_feats = np.empty((num_frames, freq_dim))\n",
    "    for i in range(num_frames):\n",
    "        frame_spliced = np.zeros((time_dim, freq_dim))\n",
    "        frame_spliced[left_context - min(i, left_context):left_context, :] = feats_numpy[i - min(i, left_context):i, :]\n",
    "        frame_spliced[left_context, :] = feats_numpy[i, :]\n",
    "        frame_spliced[left_context + 1:left_context + 1 + min(num_frames - i - 1, right_context), :] = feats_numpy[i + 1:i + 1 + min(num_frames - i - 1, right_context), :]\n",
    "        frame_tensor = Variable(torch.FloatTensor(frame_spliced))\n",
    "\n",
    "        if model_type == \"ae\":\n",
    "            recon_frames = model.forward_decoder(frame_tensor, decoder_class)\n",
    "        elif model_type == \"vae\":\n",
    "            recon_frames, mu, logvar = model.forward_decoder(frame_tensor, decoder_class)\n",
    "        else:\n",
    "            print(\"Unrecognized model type %s\" % model_type)\n",
    "        recon_frames_numpy = recon_frames.cpu().data.numpy().reshape((-1, freq_dim))\n",
    "        decoded_feats[i, :] = recon_frames_numpy[left_context:left_context + 1, :]\n",
    "    return decoded_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a86b76481943ad81fa778c5261c0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotParallelUtts>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# color_map = \"coolwarm\"\n",
    "color_map = \"viridis\"\n",
    "def plotParallelUtts(utt_id_idx):\n",
    "    fig, axarr = plt.subplots(10, sharex=True)\n",
    "    \n",
    "    fig.set_size_inches(8.5, 22)\n",
    "    \n",
    "    # CLEAN baseline\n",
    "    clean_baseline_utt_id = clean_baseline.utt_ids[utt_id_idx]\n",
    "    clean_baseline_feats = clean_baseline.feats_for_uttid(clean_baseline_utt_id)\n",
    "    axarr[0].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[0].imshow(np.transpose(clean_baseline_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[0].set_title(\"CLEAN\")\n",
    "    \n",
    "    # DIRTY baseline\n",
    "    dirty_baseline_utt_id = dirty_baseline.utt_ids[utt_id_idx]\n",
    "    dirty_baseline_feats = dirty_baseline.feats_for_uttid(dirty_baseline_utt_id)\n",
    "    axarr[1].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[1].imshow(np.transpose(dirty_baseline_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[1].set_title(\"DIRTY\")\n",
    "    \n",
    "    # CLEAN->CLEAN\n",
    "    clean_clean_feats = augmentFeats(model, clean_baseline_feats, \"clean\")\n",
    "    axarr[2].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[2].imshow(np.transpose(clean_clean_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[2].set_title(\"CLEAN>CLEAN\")\n",
    "    \n",
    "    # CLEAN->DIRTY\n",
    "    clean_dirty_feats = augmentFeats(model, clean_baseline_feats, \"dirty\")\n",
    "    axarr[3].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[3].imshow(np.transpose(clean_dirty_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[3].set_title(\"CLEAN>DIRTY\")\n",
    "    \n",
    "    # CLEAN transform difference\n",
    "    clean_transform_diff = clean_dirty_feats - clean_clean_feats\n",
    "    axarr[4].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[4].imshow(np.transpose(clean_transform_diff), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[4].set_title(\"CLEAN>DIRTY and CLEAN>CLEAN difference\")\n",
    "    \n",
    "    # DIRTY->DIRTY\n",
    "    dirty_dirty_feats = augmentFeats(model, dirty_baseline_feats, \"dirty\")\n",
    "    axarr[5].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[5].imshow(np.transpose(dirty_dirty_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[5].set_title(\"DIRTY>DIRTY\")\n",
    "    \n",
    "    # DIRTY->CLEAN\n",
    "    dirty_clean_feats = augmentFeats(model, dirty_baseline_feats, \"clean\")\n",
    "    axarr[6].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[6].imshow(np.transpose(dirty_clean_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[6].set_title(\"DIRTY>CLEAN\")\n",
    "    \n",
    "    # DIRTY transform difference\n",
    "    dirty_transform_diff = dirty_clean_feats - dirty_dirty_feats\n",
    "    axarr[7].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[7].imshow(np.transpose(dirty_transform_diff), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[7].set_title(\"DIRTY>CLEAN and DIRTY>DIRTY difference\")\n",
    "    \n",
    "    # CLEAN backtranslated\n",
    "    clean_backtrans_feats = augmentFeats(model, clean_dirty_feats, \"clean\")\n",
    "    axarr[8].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[8].imshow(np.transpose(clean_backtrans_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[8].set_title(\"CLEAN backtranslated\")\n",
    "    \n",
    "    # DIRTY backtranslated\n",
    "    dirty_backtrans_feats = augmentFeats(model, dirty_clean_feats, \"dirty\")\n",
    "    axarr[9].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[9].imshow(np.transpose(dirty_backtrans_feats), origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[9].set_title(\"DIRTY backtranslated\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # fig.savefig(\"%s_idx%d.eps\" % (clean_baseline_utt_id, utt_id_idx))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "interact(plotParallelUtts, utt_id_idx=range(len(clean_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def realOrFake(model, feats, decoder_class):\n",
    "    feats_numpy = feats.reshape((-1, freq_dim))\n",
    "    num_frames = feats_numpy.shape[0]\n",
    "    real_confidence = np.empty((1, num_frames))\n",
    "    for i in range(num_frames):\n",
    "        frame_spliced = np.zeros((time_dim, freq_dim))\n",
    "        frame_spliced[left_context - min(i, left_context):left_context, :] = feats_numpy[i - min(i, left_context):i, :]\n",
    "        frame_spliced[left_context, :] = feats_numpy[i, :]\n",
    "        frame_spliced[left_context + 1:left_context + 1 + min(num_frames - i - 1, right_context), :] = feats_numpy[i + 1:i + 1 + min(num_frames - i - 1, right_context), :]\n",
    "        frame_tensor = Variable(torch.FloatTensor(frame_spliced))\n",
    "\n",
    "        confidence = model.forward_gan(frame_tensor, decoder_class)\n",
    "        confidence_numpy = confidence.cpu().data.numpy().reshape((-1))[0]\n",
    "        real_confidence[0, i] = confidence_numpy\n",
    "    return real_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifications(confidences):\n",
    "    classifications_bool = (confidences >= 0.5)\n",
    "    return classifications_bool.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cac4aa8501d4dd2b902e693a7b4323b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotRealOrFake>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesNoColorMap = \"RdYlGn\"\n",
    "def plotRealOrFake(utt_id_idx):\n",
    "    fig, axarr = plt.subplots(6, sharex=True)\n",
    "    \n",
    "    fig.set_size_inches(8.5, 11)\n",
    "    \n",
    "    # CLEAN baseline\n",
    "    clean_baseline_utt_id = clean_baseline.utt_ids[utt_id_idx]\n",
    "    clean_baseline_feats = clean_baseline.feats_for_uttid(clean_baseline_utt_id)\n",
    "    axarr[0].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[0].imshow(classifications(realOrFake(model, clean_baseline_feats, \"clean\")), origin='lower', cmap=yesNoColorMap, aspect='auto', interpolation='none', vmin=0.0, vmax=1.0)\n",
    "    axarr[0].set_title(\"CLEAN\")\n",
    "    \n",
    "    # DIRTY baseline\n",
    "    dirty_baseline_utt_id = dirty_baseline.utt_ids[utt_id_idx]\n",
    "    dirty_baseline_feats = dirty_baseline.feats_for_uttid(dirty_baseline_utt_id)\n",
    "    axarr[1].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[1].imshow(classifications(realOrFake(model, dirty_baseline_feats, \"dirty\")), origin='lower', cmap=yesNoColorMap, aspect='auto', interpolation='none', vmin=0.0, vmax=1.0)\n",
    "    axarr[1].set_title(\"DIRTY\")\n",
    "    \n",
    "    # CLEAN->CLEAN\n",
    "    clean_clean_feats = augmentFeats(model, clean_baseline_feats, \"clean\")\n",
    "    axarr[2].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[2].imshow(classifications(realOrFake(model, clean_clean_feats, \"clean\")), origin='lower', cmap=yesNoColorMap, aspect='auto', interpolation='none', vmin=0.0, vmax=1.0)\n",
    "    axarr[2].set_title(\"CLEAN>CLEAN\")\n",
    "    \n",
    "    # CLEAN->DIRTY\n",
    "    clean_dirty_feats = augmentFeats(model, clean_baseline_feats, \"dirty\")\n",
    "    axarr[3].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[3].imshow(classifications(realOrFake(model, clean_dirty_feats, \"dirty\")), origin='lower', cmap=yesNoColorMap, aspect='auto', interpolation='none', vmin=0.0, vmax=1.0)\n",
    "    axarr[3].set_title(\"CLEAN>DIRTY\")\n",
    "    \n",
    "    # DIRTY->DIRTY\n",
    "    dirty_dirty_feats = augmentFeats(model, dirty_baseline_feats, \"dirty\")\n",
    "    axarr[4].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[4].imshow(classifications(realOrFake(model, dirty_dirty_feats, \"dirty\")), origin='lower', cmap=yesNoColorMap, aspect='auto', interpolation='none', vmin=0.0, vmax=1.0)\n",
    "    axarr[4].set_title(\"DIRTY>DIRTY\")\n",
    "    \n",
    "    # DIRTY->CLEAN\n",
    "    dirty_clean_feats = augmentFeats(model, dirty_baseline_feats, \"clean\")\n",
    "    axarr[5].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[5].imshow(classifications(realOrFake(model, dirty_clean_feats, \"clean\")), origin='lower', cmap=yesNoColorMap, aspect='auto', interpolation='none', vmin=0.0, vmax=1.0)\n",
    "    axarr[5].set_title(\"DIRTY>CLEAN\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(plotRealOrFake, utt_id_idx=range(len(clean_baseline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF DATA AUGMENTATION ALREADY RAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up augmented dev datasets\n"
     ]
    }
   ],
   "source": [
    "# Set up augmented datasets for CLEAN->CLEAN, CLEAN->DIRTY, DIRTY->DIRTY, DIRTY->CLEAN (dev set only)\n",
    "model_type = \"ae\"\n",
    "adversarial = True\n",
    "gan=False\n",
    "augmented_data_base_dir = \"/data/sls/scratch/atitus5/meng/augmented_data/cnn/%s/%s\" % (dataset,\n",
    "                                                                                       expt_name)\n",
    "\n",
    "if adversarial:\n",
    "    augmented_data_dir = \"%s/adversarial_fc_%s_act_%s_%s_ratio%s\" % (augmented_data_base_dir,\n",
    "                                                                     adv_fc_delim,\n",
    "                                                                     adv_activation,\n",
    "                                                                     model_type,\n",
    "                                                                     str(noise_ratio))\n",
    "elif gan:\n",
    "    augmented_data_dir = \"%s/gan_fc_%s_act_%s_%s_ratio%s\" % (augmented_data_base_dir,\n",
    "                                                                     gan_fc_delim,\n",
    "                                                                     gan_activation,\n",
    "                                                                     model_type,\n",
    "                                                                     str(noise_ratio))\n",
    "else:\n",
    "    augmented_data_dir = \"%s/%s_ratio%s\" % (augmented_data_base_dir,\n",
    "                                            model_type,\n",
    "                                            str(noise_ratio))\n",
    "\n",
    "clean_clean = KaldiEvalDataset(os.path.join(augmented_data_dir, \"dev-src_clean-tar_clean.scp\"))\n",
    "clean_dirty = KaldiEvalDataset(os.path.join(augmented_data_dir, \"dev-src_clean-tar_dirty.scp\"))\n",
    "dirty_dirty = KaldiEvalDataset(os.path.join(augmented_data_dir, \"dev-src_dirty-tar_dirty.scp\"))\n",
    "dirty_clean = KaldiEvalDataset(os.path.join(augmented_data_dir, \"dev-src_dirty-tar_clean.scp\"))\n",
    "print(\"Set up augmented dev datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f34c2227fab4090bc0723988107caa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotParallelUtts>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# color_map = \"coolwarm\"\n",
    "color_map = \"viridis\"\n",
    "def plotParallelUtts(utt_id_idx):\n",
    "    fig, axarr = plt.subplots(6, sharex=True)\n",
    "    \n",
    "    fig.set_size_inches(8.5, 11)\n",
    "    \n",
    "    # CLEAN baseline\n",
    "    clean_baseline_utt_id = clean_baseline.utt_ids[utt_id_idx]\n",
    "    clean_baseline_feats = np.transpose(clean_baseline.feats_for_uttid(clean_baseline_utt_id))\n",
    "    axarr[0].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[0].imshow(clean_baseline_feats, origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[0].set_title(\"CLEAN\")\n",
    "    \n",
    "    # DIRTY baseline\n",
    "    dirty_baseline_utt_id = dirty_baseline.utt_ids[utt_id_idx]\n",
    "    dirty_baseline_feats = np.transpose(dirty_baseline.feats_for_uttid(dirty_baseline_utt_id))\n",
    "    axarr[1].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[1].imshow(dirty_baseline_feats, origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[1].set_title(\"DIRTY\")\n",
    "    \n",
    "    # CLEAN->CLEAN baseline\n",
    "    clean_clean_utt_id = clean_clean.utt_ids[utt_id_idx]\n",
    "    clean_clean_feats = np.transpose(clean_clean.feats_for_uttid(clean_clean_utt_id))\n",
    "    axarr[2].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[2].imshow(clean_clean_feats, origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[2].set_title(\"CLEAN>CLEAN\")\n",
    "    \n",
    "    # CLEAN->DIRTY baseline\n",
    "    clean_dirty_utt_id = clean_dirty.utt_ids[utt_id_idx]\n",
    "    clean_dirty_feats = np.transpose(clean_dirty.feats_for_uttid(clean_dirty_utt_id))\n",
    "    axarr[3].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[3].imshow(clean_dirty_feats, origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[3].set_title(\"CLEAN>DIRTY\")\n",
    "    \n",
    "    # DIRTY->DIRTY baseline\n",
    "    dirty_dirty_utt_id = dirty_dirty.utt_ids[utt_id_idx]\n",
    "    dirty_dirty_feats = np.transpose(dirty_dirty.feats_for_uttid(dirty_dirty_utt_id))\n",
    "    axarr[4].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[4].imshow(dirty_dirty_feats, origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[4].set_title(\"DIRTY>DIRTY\")\n",
    "    \n",
    "    # DIRTY->CLEAN baseline\n",
    "    dirty_clean_utt_id = dirty_clean.utt_ids[utt_id_idx]\n",
    "    dirty_clean_feats = np.transpose(dirty_clean.feats_for_uttid(dirty_clean_utt_id))\n",
    "    axarr[5].axis('off')    # Pretty-up the resulting output by removing gridlines\n",
    "    axarr[5].imshow(dirty_clean_feats, origin='lower', cmap=color_map, aspect='auto', interpolation='none')\n",
    "    axarr[5].set_title(\"DIRTY>CLEAN\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # fig.savefig(\"%s_idx%d.eps\" % (clean_baseline_utt_id, utt_id_idx))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "interact(plotParallelUtts, utt_id_idx=range(len(clean_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
