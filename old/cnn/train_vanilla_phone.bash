#!/bin/bash
#SBATCH -p sm
#SBATCH -n1
#SBATCH -N1-1
#SBATCH -c 16
#SBATCH --gres=gpu:1
#SBATCH --mem=32768
#SBATCH --time=24:00:00
#SBATCH -J train_vanilla_phone

echo "STARTING VANILLA MULTIDECODER MULTITASK TRAINING JOB"

. ./path.sh
. ./cnn/base_config.sh
. ./cnn/phone_config.sh

stage=0
if [ $# -eq 1 ]; then
    stage=$1
fi
echo "Starting from stage $stage"

echo "Setting up environment..."
export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/data/sls/u/meng/skanda/cuda/lib64:$LD_LIBRARY_PATH
source activate $TRAIN_ENV
echo "Environment set up."
    
lang=$CLEAN_RECIPE/exp/tri3/graph
model=$CLEAN_RECIPE/exp/tri3_ali/final.mdl

# Check for training phones files and generate them if they don't exist
echo "Checking for training phone files..."
if [ ! -f $DIRTY_FEATS/train/phones.scp ]; then
    echo "phones.scp not found at $DIRTY_FEATS/train; generating"

    # Alignments already generated by recipe -- just convert

    # Convert each split to PDFs, then concatenate into one SCP file
    for ((split=1;split<=30;split++)); do
        # Convert to uncompressed binary ark file
        # Although it isn't truly a "feature file", we can use copy-feats by first
        # converting to Hao's batch file format
        ali-to-pdf $model ark:"gunzip -c $CLEAN_RECIPE/exp/tri3_ali/ali.$split.gz |" ark,t:- \
            | python3 $UTILS/ali2batch.py \
            | python3 $UTILS/batch2ark.py \
            | copy-feats --compress=false ark:- ark,scp:$DIRTY_FEATS/train/phones.$split.ark,$DIRTY_FEATS/train/phones.$split.scp || exit 1
    done
    cat $DIRTY_FEATS/train/phones.*.scp | sort > $DIRTY_FEATS/train/phones.scp
fi
echo "Train phone files ready"

# Check for dev phones files and generate them if they don't exist
echo "Checking for dev phone files..."
if [ ! -f $DIRTY_FEATS/dev/phones.scp ]; then
    echo "phones.scp not found at $DIRTY_FEATS/dev; generating"

    # Alignments NOT generated by recipe -- need to decode first and get alignments
    align_nj=1
    $STEPS/align_fmllr.sh --nj "$align_nj" --cmd $UTILS/run.pl \
        $CLEAN_RECIPE/data/dev $CLEAN_RECIPE/data/lang $CLEAN_RECIPE/exp/tri3 $DIRTY_FEATS/dev/tri3_ali

    # Convert to PDFs (as uncompressed binary ark file)
    # Although it isn't truly a "feature file", we can use copy-feats by first
    # converting to Hao's batch file format
    ali-to-pdf $model ark:"gunzip -c $DIRTY_FEATS/dev/tri3_ali/ali.1.gz |" ark,t:- \
        | python3 $UTILS/ali2batch.py \
        | python3 $UTILS/batch2ark.py \
        | copy-feats --compress=false ark:- ark,scp:$DIRTY_FEATS/dev/phones.ark,$DIRTY_FEATS/dev/phones.scp || exit 1
fi
echo "Dev phone files ready"

train_log=$LOG_DIR/train_vanilla_phone.log
if [ -f $train_log ]; then
    # Move old log
    mv $train_log $LOG_DIR/train_vanilla_phone-$(date +"%F_%T%z").log
fi

# Train model itself
if [ $stage -lt 1 ]; then
    echo "Training model..."
    python3 cnn/scripts/train_vanilla_phone.py &> $train_log
fi

if [ $stage -lt 2 ]; then
    for source_class in "${DECODER_CLASSES[@]}"; do
        echo "Processing source class $source_class"

        for subdir in dev test; do
            echo "Scoring data in $subdir"
            
            dir=$MULTITASK_DIR/$subdir/$source_class
            mkdir -p $dir

            if [ ! -f $dir/lat.1.gz ]; then
                # Evaluate model, print log probabilities and generate lattices
                # Use only one job since TIMIT is fairly small
                echo "Generating lattice..."
                python3 cnn/scripts/eval_vanilla_phone.py $subdir $source_class \
                    | $UTILS/batch2ark.py \
                    | latgen-faster-mapped \
                    --max-active=7000 \
                    --beam=15 \
                    --lattice-beam=7 \
                    --acoustic-scale=0.1 \
                    --allow-partial=true \
                    --word-symbol-table=$lang/words.txt \
                    $model \
                    $lang/HCLG.fst \
                    ark:- \
                    "ark:|gzip -c > $dir/lat.1.gz" \
                    2> $dir/decode-1.log
            else
                echo "Lattice already generated; skipping"
            fi

            # Score results
            echo "Scoring $subdir results for $source_class..."
            num_jobs=10
            echo $num_jobs > $dir/num_jobs
            local/score.sh $CLEAN_RECIPE/data/$subdir $lang $dir $model >> $train_log 2>&1

            # Print best results
            echo "RESULTS FOR $subdir, class $source_class"
            grep WER $dir/wer_* 2>/dev/null | $UTILS/best_wer.sh
            grep Sum $dir/score_*/*.sys 2>/dev/null | $UTILS/best_wer.sh
        done
    done
fi

echo "DONE VANILLA MULTIDECODER MULTITASK TRAINING JOB"
